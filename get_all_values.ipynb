{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7314e-aeaf-41ad-a7a3-715151fc5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import ee\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "from sepal_ui import aoi\n",
    "from sepal_ui import sepalwidgets as sw\n",
    "from sepal_ui.scripts import gee\n",
    "from osgeo import gdal\n",
    "import rasterio as rio\n",
    "\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from component import parameter as pm\n",
    "from component.scripts import gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c7b49-f833-48fa-8b6c-aa92550255aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47991088-275b-464f-b23f-63d3b475dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_handler = gdrive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc09bf8-ff94-41ca-9d05-89a10574718f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load all the data provided in se.plan as a csv \n",
    "\n",
    "This proces has been setup to extract from earthengine all the information required to run se.plan analysis in STATA a software that is not intended for Geospatial data. Here we will export as a single CSV all non mask pixels of 1km from se.plan layers + their lattitude, longitude, surface and administrative names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06650d19-527f-4480-ae17-df5a679bcc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the downloading path\n",
    "save_dir = Path.home() / \"module_results\" / \"se.plan_csv\"\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "raw_dir = save_dir / \"raw_data\"\n",
    "raw_dir.mkdir(exist_ok=True)\n",
    "\n",
    "csv_dir = save_dir / \"raw_csv\"\n",
    "csv_dir.mkdir(exist_ok=True)\n",
    "\n",
    "dafatrames_dir = save_dir / \"raw_final\"\n",
    "dafatrames_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18eb19-eebc-474f-8062-2f41e476d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the country list\n",
    "lmic_list = pd.read_csv(pm.country_list)\n",
    "\n",
    "lmic_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1af4e-bf0b-4f6c-aca8-f1d2f7c0de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the layers\n",
    "layer_list = pd.read_csv(\n",
    "    pm.layer_list, usecols=[\"layer_id\", \"theme\", \"gee_asset\"]\n",
    ").sort_values(by=[\"theme\"])\n",
    "\n",
    "layer_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ad8e1-c532-4925-8c82-4c875facfcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the reference parameters\n",
    "name = \"treecover_with_potential\"\n",
    "layer = layer_list[layer_list.layer_id == name].iloc[0]\n",
    "\n",
    "ee_ref = ee.Image(layer.gee_asset)\n",
    "ee_ref_crs = ee_ref.projection()\n",
    "\n",
    "# the image is not bounded any more I need to draw it manually\n",
    "# ee_ref_geom = ee_ref.geometry()\n",
    "ee_ref_geom = ee.Geometry.Polygon(\n",
    "    coords=[\n",
    "        [-144.51600549814273, -58.07221137655366],\n",
    "        [192.42150308492614, -58.07221137655366],\n",
    "        [192.42150308492614, 57.78491418812651],\n",
    "        [-144.51600549814273, 57.78491418812651],\n",
    "        [-144.51600549814273, -58.07221137655366],\n",
    "    ],\n",
    "    geodesic=False,\n",
    "    proj=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "\n",
    "def export_to_drive(name, image):\n",
    "    \"\"\"\n",
    "    Export image to rive using the ref parameters\n",
    "    If the image is not already exported/exporting\n",
    "\n",
    "    Args:\n",
    "        name (str): the name of the asset\n",
    "        image (ee.Image): the image to export\n",
    "    \"\"\"\n",
    "\n",
    "    if not any(\n",
    "        [\n",
    "            len(drive_handler.get_files(f\"se.plan/{name}\")),\n",
    "            (save_dir / f\"{name}.vrt\").is_file(),\n",
    "            gee.is_running(name),\n",
    "        ]\n",
    "    ):\n",
    "        task_config = {\n",
    "            \"folder\": \"se.plan\",\n",
    "            \"image\": image,\n",
    "            \"description\": name,\n",
    "            \"region\": ee_ref_geom,\n",
    "            \"scale\": 1000,\n",
    "            \"crs\": ee_ref_crs,\n",
    "            \"maxPixels\": 10e12,\n",
    "        }\n",
    "\n",
    "        task = ee.batch.Export.image.toDrive(**task_config)\n",
    "        task.start()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9784b-e818-473a-9797-223b2e26598d",
   "metadata": {},
   "source": [
    "## extract the LMIC\n",
    "\n",
    "The low and medium income countries are the default target countries of se.plan. thus this process will only retrieve information from these countries. The shapes will be exported as a geopackage file that can be read by any GIS software. The geometry will also be exported as an image to be used in the csv further down the script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4402e55c-3aa7-4d0b-9add-3a8b2bbb52af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the sepal_ui defined aoi_model object to retreive aoi from GADM using their country code\n",
    "aoi_model = aoi.AoiModel(alert=sw.Alert(), gee=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607cd8d-22fa-4053-a7bc-f630f9c14634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sublist of countries using the iso_3 code based on the aoi_model file\n",
    "country_list = pd.read_csv(aoi_model.FILE[0])  # GADM based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da15363-ac08-4c2a-8160-292ed59fc27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def level0_gdf(admin):\n",
    "    \"\"\"\n",
    "    return a clomplete gdf for each country at level 0 but including the level 1 information\n",
    "\n",
    "    a function to create a composite gdf from the gadm exported countries\n",
    "    the main trick is to read all the countries even the one that have no level 1 data\n",
    "\n",
    "    Args:\n",
    "        admin (int): the admin number GID_0\n",
    "\n",
    "    Return:\n",
    "        (goedataframe): the geodataframe of the country\n",
    "    \"\"\"\n",
    "\n",
    "    # get all the sub administrative areas\n",
    "    df = country_list[country_list.GID_0 == admin]  # only the featured country\n",
    "    df = df.drop_duplicates(subset=\"GID_1\")  # remove all GID_2\n",
    "\n",
    "    # get all the admin numbers\n",
    "    gdf = None\n",
    "    for i, row in df.iterrows():\n",
    "\n",
    "        admin = row.GID_1 if row.GID_1 and row.GID_1 == row.GID_1 else row.GID_0\n",
    "\n",
    "        aoi_model.admin = admin\n",
    "        aoi_model.set_object(method=\"ADMIN0\")  # any admin level do the same\n",
    "\n",
    "        tmp_gdf = aoi_model.gdf\n",
    "        gdf = tmp_gdf if gdf is None else pd.concat([gdf, tmp_gdf])\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2893690-ec39-4e33-a76f-7e6176810eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the countries as a geopackage\n",
    "# skip this step if the countries have already been exported\n",
    "gid_path = save_dir / \"lmic_leve1.gpkg\"\n",
    "\n",
    "if gid_path.is_file():\n",
    "\n",
    "    # read the file\n",
    "    gdf = gpd.read_file(gid_path, layer=\"GID\")\n",
    "\n",
    "else:\n",
    "    gdf = None\n",
    "    with tqdm(total=len(lmic_list), desc=\"loading countries\") as pbar:\n",
    "        for i, row in lmic_list.iterrows():\n",
    "\n",
    "            tmp_gdf = level0_gdf(row.ISO3)\n",
    "            gdf = tmp_gdf if gdf is None else pd.concat([gdf, tmp_gdf])\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    gdf.to_file(gid_path, driver=\"GPKG\", layer=\"GID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a082f-b995-439a-925d-44a8e03efcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all the countries are included\n",
    "if len(gdf.GID_0.unique()) != 139:\n",
    "    raise ValeuError(\n",
    "        f\"Their are 139 LMIC countries which is not corresponding to the {len(gdf.GID_0.unique())} provided\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7fda3-2900-43cd-a71e-1568419da1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the countries\n",
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac76ce-2dc9-41e6-87d6-323ce11416de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export an admin level as a gee image to drive\n",
    "def export_admin(level):\n",
    "    \"\"\"\n",
    "    Export a specific level of the lmic country list as an image aligned on the reference\n",
    "\n",
    "    Args:\n",
    "        level (int): the admin level value to export\n",
    "    \"\"\"\n",
    "\n",
    "    layer = \"projects/john-ee-282116/assets/fao-restoration/features/rest_pot_gt_treecoverfrac_mask_urban\"\n",
    "    ee_ref_crs = ee.Projection(ee.Image(layer).projection())\n",
    "    ee_lmic_list = ee.List(list(lmic_list.GAUL.values))\n",
    "\n",
    "    ee_lmic = (\n",
    "        ee.FeatureCollection(\"FAO/GAUL/2015/level1\")\n",
    "        .filter(ee.Filter.inList(\"ADM0_CODE\", ee_lmic_list))\n",
    "        .reduceToImage(properties=[f\"ADM{level}_CODE\"], reducer=ee.Reducer.first())\n",
    "        .select(\"first\")\n",
    "        .rename(f\"ADM{level}_CODE\")\n",
    "        .setDefaultProjection(crs=ee_ref_crs)\n",
    "        .reduceResolution(reducer=ee.Reducer.mode(), maxPixels=2048)\n",
    "    )\n",
    "\n",
    "    export_to_drive(f\"ADM{level}_CODE\", ee_lmic)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# add the 2 admin export to the database\n",
    "# for i in range(2):\n",
    "#    name = f\"ADM{i}_CODE\"\n",
    "#    # buged at the moment I did it manually (https://code.earthengine.google.com/cb4d25f32e0d9f94c7ce68263d752d19)\n",
    "#    export_admin(i)\n",
    "#    if not name in layer_list.layer_id:\n",
    "#        layer_list.loc[len(layer_list)] = [f\"ADM{i}_CODE\", f\"ADM{i}_CODE\", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056685de-5843-4f3f-ba50-1d8883265fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add potential_treecover to layers\n",
    "layer_list.loc[len(layer_list)] = [\n",
    "    \"potential_treecover\",\n",
    "    \"\",\n",
    "    \"users/yelenafinegold/bastin2019/restoration_potential\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2520a95-eeea-4164-8652-a3819a628f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2261b11f-ce65-4634-9502-de60dadb8966",
   "metadata": {},
   "source": [
    "## extract the pixel area\n",
    "\n",
    "It's meaningfull to compute the pixel area as the dataset covers the entire world "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f9765a-fa5c-4b01-b4a9-d676d623b2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pixel sizes of the reference as an image\n",
    "pixel_area = \"pixel_area\"\n",
    "water = ee.Image(\"JRC/GSW1_3/GlobalSurfaceWater\")\n",
    "land_proportion = (\n",
    "    water.select(\"transition\")\n",
    "    .mask()\n",
    "    .Not()\n",
    "    .mask(water.select(\"max_extent\").mask())\n",
    "    # .multiply(ee.Image.pixelArea())\n",
    "    .reduceResolution(reducer=ee.Reducer.mean(), maxPixels=2048)\n",
    ")\n",
    "export_to_drive(pixel_area, land_proportion)\n",
    "\n",
    "# add the layer to the list\n",
    "if not pixel_area in layer_list.layer_id:\n",
    "    layer_list.loc[len(layer_list)] = [pixel_area, pixel_area, \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c19bf-fd10-410c-8651-002c8d909663",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f75f2-bfea-40c0-a391-f9464f85ebbf",
   "metadata": {},
   "source": [
    "## export the 24 normal layers \n",
    "\n",
    "Now we are launhing the exportation of all the layer included in se.plan with as gDrive files. The exportation will be monitored here. the next cells cannot be displayed until this step is finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd5df0-3f79-404c-b13f-0f3baff31c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download treecover files, add it in the same directoryand to the layer list\n",
    "treecover = \"current_treecover\"\n",
    "ee_treecover = (\n",
    "    ee.Image(\"COPERNICUS/Landcover/100m/Proba-V-C3/Global/2019\")\n",
    "    .select(\"tree-coverfraction\")\n",
    "    .reduceResolution(reducer=ee.Reducer.mean(), maxPixels=2048)\n",
    ")\n",
    "\n",
    "export_to_drive(treecover, ee_treecover)\n",
    "\n",
    "# add the layer to the list\n",
    "if not treecover in layer_list.layer_id:\n",
    "    layer_list.loc[len(layer_list)] = [treecover, treecover, \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc1d94-400f-4426-988f-74258c32e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the original layers\n",
    "original_layers = layer_list[layer_list.gee_asset != \"\"]\n",
    "len(original_layers)\n",
    "original_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c4fc28-b3ab-4e9d-afb9-0fc1a4f4c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_layer = layer_list[layer_list.layer_id.isin([\"potential_treecover\"])]\n",
    "filter_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd977e3-af61-4ebf-bdaa-e49f9a7a04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export all the layers with the adapted reducer\n",
    "with tqdm(total=len(filter_layer), desc=\"export original layers\") as pbar:\n",
    "    for i, row in filter_layer.iterrows():\n",
    "\n",
    "        # default behaviour\n",
    "        ee_reducer = ee.Reducer.mean()\n",
    "\n",
    "        # main mask\n",
    "        if row.layer_id in [\"treecover_with_potential\"]:\n",
    "            ee_reducer = ee.Reducer.anyNonZero()\n",
    "\n",
    "        # most frequent value\n",
    "        elif row.layer_id in [\n",
    "            \"protected_areas\",\n",
    "            \"ecozones\",\n",
    "            \"land_cover\",\n",
    "            \"protected_areas\",\n",
    "            \"declining_population\",\n",
    "        ]:\n",
    "            ee_reducer = ee.Reducer.mode()\n",
    "\n",
    "        image = ee.Image(row.gee_asset.strip())\n",
    "\n",
    "        # export\n",
    "        ee_image = (\n",
    "            ee.Image(row.gee_asset.strip())\n",
    "            .select(0)\n",
    "            .reduceResolution(reducer=ee_reducer, maxPixels=2048)\n",
    "        )\n",
    "        export_to_drive(row.layer_id, ee_image)\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "print(\n",
    "    \"You can now monitor your exporting steps from earthegine: https://code.earthengine.google.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb3255-b347-4cf5-99d8-ae37bd64fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# automatic waiting process #\n",
    "#############################\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b217130-d8c1-4d6b-9202-b4e19fb68a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list.loc[len(layer_list)] = [\"lat\", \"lat\", \"\"]\n",
    "layer_list.loc[len(layer_list)] = [\"lon\", \"lon\", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b62eb-05fc-4b41-a5db-314defbe9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once the image are available on my drive\n",
    "# download theme as files\n",
    "with tqdm(total=len(filter_layer), desc=\"retrieve from GDRIVE\") as pbar:\n",
    "    for i, row in filter_layer.iterrows():\n",
    "\n",
    "        vrt_path = save_dir / f\"{row.layer_id}.vrt\"\n",
    "\n",
    "        if vrt_path.is_file():\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        files = drive_handler.get_files(row.layer_id)\n",
    "        files = [f for f in files if \"d3_slope\" not in f[\"name\"]]\n",
    "        if len(files):\n",
    "            loc_files = drive_handler.download_files(files, raw_dir)\n",
    "            # drive_handler.delete_files(files)\n",
    "\n",
    "            # create a vrt to manipulate everything\n",
    "            ds = gdal.BuildVRT(str(vrt_path), [str(f) for f in loc_files])\n",
    "            ds.FlushCache()\n",
    "\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f944d1d-1fe9-4993-9666-483921ca8d63",
   "metadata": {},
   "source": [
    "# create the lat and lon files \n",
    "\n",
    "as the final file will be an csv lat and lon of the data need to be extracted. To do that we will simply extract the resolution of the ref layer and create a grid out of it. finally we add the pixelSize for each row and column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ddd68-389c-4889-9ee7-3fad833178fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the reference file characteristics\n",
    "ref_file = save_dir / \"treecover_with_potential.vrt\"\n",
    "with rio.open(ref_file) as mask_f:\n",
    "\n",
    "    pixelSizes = mask_f.res\n",
    "    mask_shape = mask_f.read(1).shape\n",
    "    kwargs = mask_f.meta.copy()\n",
    "    grid = np.indices(mask_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53504951-8833-47a5-b01b-7b4cddab978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def export_corrdinates(name, column):\n",
    "#    \"\"\"\n",
    "#    export the lat and lon corrdinates of the center of the pixel as images\n",
    "#\n",
    "#    Args:\n",
    "#        name (str): the name of the output\n",
    "#        column (str): either or not the value is the coumn of the grid\n",
    "#\n",
    "#    Return:\n",
    "#        (path): the raw file\n",
    "#    \"\"\"\n",
    "#\n",
    "#\n",
    "#    coord = ee.Image.pixelLonLat().select(\"longitude\")\n",
    "#\n",
    "#    Export.image.toDrive({\n",
    "#  image: lat,\n",
    "#  folder: \"se.plan\",\n",
    "#  description: \"lon\",\n",
    "#  region: ee_ref_geom,\n",
    "#  scale: 1000,\n",
    "#  crs: \"EPSG:4326\",\n",
    "#  maxPixels: 10e12\n",
    "# })\n",
    "#\n",
    "#    raw_file = save_dir / \"raw_data\" / f\"{name}.tif\"\n",
    "#\n",
    "#    if raw_file.is_file():\n",
    "#        return raw_file\n",
    "#\n",
    "#    data = grid[column] * pixelSizes[column] + pixelSizes[column] / 2\n",
    "#\n",
    "#    dtype = rio.dtypes.get_minimum_dtype(data)\n",
    "#    kwargs[\"dtype\"] = dtype\n",
    "#    kwargs[\"driver\"] = \"GTiff\"\n",
    "#\n",
    "#    with rio.open(raw_file, \"w\", **kwargs) as dst:\n",
    "#        dst.write(data.astype(dtype), 1)\n",
    "#\n",
    "#    return raw_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ecb48-020a-40e8-93e7-e8138a4f54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate lat and long files\n",
    "coords = [\n",
    "    {\"name\": \"lon\", \"column\": 0},\n",
    "    {\"name\": \"lat\", \"column\": 1},\n",
    "]\n",
    "\n",
    "for coord in coords:\n",
    "\n",
    "    name = coord[\"name\"]\n",
    "\n",
    "    raw_file = export_corrdinates(**coord)\n",
    "\n",
    "    vrt_file = save_dir / f\"{name}.vrt\"\n",
    "    ds = gdal.BuildVRT(str(vrt_file), [str(raw_file)])\n",
    "    ds.FlushCache()\n",
    "\n",
    "    if not name in layer_list.layer_id.values:\n",
    "        layer_list.loc[len(layer_list)] = [name, name, \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac797faf-843f-45df-80ae-9e846ca00c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(layer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0f87e-09a4-4372-b7f0-4da27b80fd45",
   "metadata": {},
   "source": [
    "## assemble all vrts as a single dataframe\n",
    "\n",
    "to export to csv every vrts are assembled as a single dataframe \n",
    "then we exclude all the rows with masked by the ref image \n",
    "and we export as csv to the save_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f048ef-9900-475b-8184-cfbeb0f6bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list = layer_list[layer_list.layer_id.isin([\"ADM0_CODE\", \"ADM1_CODE\"])]\n",
    "filter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848dca7a-8d3a-43bf-b603-bebae3849760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data mask\n",
    "ref_file = save_dir / \"treecover_with_potential.vrt\"\n",
    "with rio.open(ref_file) as mask_f:\n",
    "\n",
    "    # do it on a window for testing purposes\n",
    "    # window = mask_f.block_window(1, 0, 0)\n",
    "\n",
    "    mask = mask_f.read(1)\n",
    "    mask = mask != 0\n",
    "\n",
    "with tqdm(total=len(filter_layer), desc=\"extract data by layer\") as pbar:\n",
    "    for _, r in filter_layer.iterrows():\n",
    "        name = r.layer_id\n",
    "\n",
    "        vrt_file = save_dir / f\"{name}.vrt\"\n",
    "        df_file = csv_dir / f\"{name}.csv\"\n",
    "\n",
    "        if not df_file.is_file():\n",
    "\n",
    "            with rio.open(vrt_file) as vrt:\n",
    "                raw = vrt.read(1)[mask].flatten()\n",
    "                np.savetxt(df_file, np.array(raw), delimiter=\",\", fmt=\"%.10f\")\n",
    "\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ee4a89-b589-4fa1-a9a2-1d70c6ecbded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate all the csv as a dataframe\n",
    "all_files = csv_dir.glob(\"*.csv\")\n",
    "\n",
    "# create the init dataframe\n",
    "# consume the first dataframe of the generator\n",
    "f = next(all_files)\n",
    "final_df = (\n",
    "    dd.read_csv(f, header=None, names=[f.stem])\n",
    "    .repartition(npartitions=500)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "final_df = final_df.assign(\n",
    "    **{\n",
    "        f.stem: dd.read_csv(f, header=None, names=[f.stem])\n",
    "        .repartition(npartitions=500)\n",
    "        .reset_index(drop=True)[f.stem]\n",
    "        for f in all_files\n",
    "    }\n",
    ")\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bbec32-321b-4f09-9df1-906bd11f7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 partitions is compatible with a r8 AWS instance\n",
    "final_file = dafatrames_dir / \"*.csv\"\n",
    "final_df.repartition(npartitions=50).to_csv(final_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9ae41-fc4f-4c03-b1b2-676059595ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48646a10-f78c-421e-9c34-a6d660a36c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# awk '(NR == 1) || (FNR > 1)' file*.csv > bigfile.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9fe55-4c91-467b-ab63-de1038678b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the 2 first lines\n",
    "with (save_dir / \"dataset.csv\").open() as f:\n",
    "    print(f.readline())\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e37fe1-0c37-418d-8802-7daf39430347",
   "metadata": {},
   "source": [
    "## Build single csv for debugging purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1165bb5-1b5b-4d84-8a05-3fce23902b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625cdf0-0e90-4615-b44e-8acc73367080",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in layer_list.iterrows():\n",
    "\n",
    "    # default behaviour\n",
    "    ee_reducer = ee.Reducer.mean()\n",
    "\n",
    "    # main mask\n",
    "    if row.layer_id in [\"treecover_with_potential\"]:\n",
    "        ee_reducer = ee.Reducer.anyNonZero()\n",
    "\n",
    "    # most frequent value\n",
    "    elif row.layer_id in [\n",
    "        \"protected_areas\",\n",
    "        \"ecozones\",\n",
    "        \"land_cover\",\n",
    "        \"protected_areas\",\n",
    "        \"declining_population\",\n",
    "    ]:\n",
    "        ee_reducer = ee.Reducer.mode()\n",
    "\n",
    "    # export\n",
    "    ee_image = (\n",
    "        ee.Image(row.gee_asset.strip())\n",
    "        .select(0)\n",
    "        .reduceResolution(reducer=ee_reducer, maxPixels=2048)\n",
    "    )\n",
    "    export_to_drive(row.layer_id, ee_image)\n",
    "\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc6e5b-a1a6-4844-8573-39ed538cbae3",
   "metadata": {},
   "source": [
    "##  extra tests for specific datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7f43d-b2bf-4b00-a7cc-072a71c1668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check lat long\n",
    "import numpy as np\n",
    "\n",
    "lat = csv_dir / \"lat.csv\"\n",
    "np_lat = np.genfromtxt(lat, delimiter=\",\")\n",
    "print(np.min(np_lat))\n",
    "print(np.max(np_lat))\n",
    "print(len(np_lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8362025-3e80-4be3-8d10-6a2430ce6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = csv_dir / \"lon.csv\"\n",
    "np_lon = np.genfromtxt(lon, delimiter=\",\")\n",
    "print(np.min(np_lon))\n",
    "print(np.max(np_lon))\n",
    "print(len(np_lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d78e06-25a3-4c9b-9a23-dff8f04af7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = csv_dir / \"treecover_with_potential.csv\"\n",
    "np_mask = np.genfromtxt(lon, delimiter=\",\")\n",
    "len(np_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc14c8-4065-4d8d-95d1-b3bceaec9f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adm0 = csv_dir / \"ADM0_CODE.csv\"\n",
    "np_adm0 = np.genfromtxt(adm0, delimiter=\",\")\n",
    "print(len(np_adm0))\n",
    "\n",
    "adm0_sepal_ui = Path(\"/home/prambaud/libs/sepal_ui/sepal_ui/scripts/gaul_database.csv\")\n",
    "print(adm0_sepal_ui.is_file())\n",
    "\n",
    "np_sepal_ui = pd.read_csv(adm0_sepal_ui).ADM0_CODE.unique()\n",
    "\n",
    "np.unique(np_adm0[~np.isin(np_adm0, np_sepal_ui)], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c10f76-dabc-45da-9327-ef6985f168fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adm1 = csv_dir / \"ADM1_CODE.csv\"\n",
    "np_adm1 = np.genfromtxt(adm1, delimiter=\",\")\n",
    "print(len(np_adm0))\n",
    "\n",
    "adm1_sepal_ui = Path(\"/home/prambaud/libs/sepal_ui/sepal_ui/scripts/gaul_database.csv\")\n",
    "print(adm1_sepal_ui.is_file())\n",
    "\n",
    "np_sepal_ui = pd.read_csv(adm1_sepal_ui).ADM1_CODE.unique()\n",
    "\n",
    "np.unique(np_adm1[~np.isin(np_adm1, np_sepal_ui)], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7b512-58ac-4605-8caf-1286f6494bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "40534489 / 106816099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790fc703-34fb-4ee7-87cf-b21ea551580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np_adm0, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4e565-bcb0-4517-b2ae-e6c149c92891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "declining_population = csv_dir / \"declining_population.csv\"\n",
    "np_declining_population = np.genfromtxt(declining_population, delimiter=\",\")\n",
    "print(len(np_declining_population))\n",
    "\n",
    "print(np.unique(np_declining_population, return_counts=True))\n",
    "\n",
    "print(107274 / 106816099)\n",
    "print(75512734 / 106816099)\n",
    "print(31196091 / 106816099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08926e60-0211-431d-aedc-0296bef3358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "slope = csv_dir / \"slope.csv\"\n",
    "np_slope = np.genfromtxt(slope, delimiter=\",\")\n",
    "print(len(np_slope))\n",
    "\n",
    "print(np.count_nonzero(np.isnan(np_slope)))\n",
    "print(np.min(np_slope))\n",
    "print(np.max(np_slope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c74f6d-d92f-4d37-878a-5896c5709ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d90d2-aa41-4d7b-8273-e25f719cad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "implementation_cost = csv_dir / \"implementation_cost.csv\"\n",
    "np_implementation_cost = np.genfromtxt(implementation_cost, delimiter=\",\")\n",
    "print(len(np_implementation_cost))\n",
    "\n",
    "print(np.count_nonzero(np.isnan(np_implementation_cost)))\n",
    "print(np.min(np_implementation_cost))\n",
    "print(np.max(np_implementation_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca05d5-f792-40e2-a567-2cde39be7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "natural_regeneration = csv_dir / \"natural_regeneration.csv\"\n",
    "np_natural_regeneration = np.genfromtxt(natural_regeneration, delimiter=\",\")\n",
    "print(len(np_natural_regeneration))\n",
    "\n",
    "print(np.count_nonzero(np.isnan(np_natural_regeneration)))\n",
    "print(np.min(np_natural_regeneration))\n",
    "print(np.max(np_natural_regeneration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5d88e-5127-4976-9b96-a66f6275ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "opportunity_cost = csv_dir / \"opportunity_cost.csv\"\n",
    "np_opportunity_cost = np.genfromtxt(opportunity_cost, delimiter=\",\")\n",
    "print(len(np_opportunity_cost))\n",
    "\n",
    "print(np.count_nonzero(np.isnan(np_opportunity_cost)))\n",
    "print(np.min(np_opportunity_cost))\n",
    "print(np.max(np_opportunity_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a25ad-9b5b-4374-9c25-608a245f8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "property_rights = csv_dir / \"property_rights.csv\"\n",
    "np_property_rights = np.genfromtxt(property_rights, delimiter=\",\")\n",
    "print(len(np_property_rights))\n",
    "\n",
    "print(np.count_nonzero(np.isnan(np_property_rights)))\n",
    "print(np.min(np_property_rights))\n",
    "print(np.max(np_property_rights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ef299-befa-49c8-b3bc-0791f4ff88fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "woodfuel_harvest = csv_dir / \"woodfuel_harvest.csv\"\n",
    "np_woodfuel_harvest = np.genfromtxt(woodfuel_harvest, delimiter=\",\")\n",
    "print(len(np_woodfuel_harvest))\n",
    "\n",
    "print(np.count_nonzero(np.isnan(np_woodfuel_harvest)))\n",
    "print(np.min(np_woodfuel_harvest))\n",
    "print(np.max(np_woodfuel_harvest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0314887-82f3-4c7d-a296-a5da7f939895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "forest_job = csv_dir / \"forest_job.csv\"\n",
    "np_forest_job = np.genfromtxt(forest_job, delimiter=\",\")\n",
    "print(len(np_forest_job))\n",
    "\n",
    "print(np.count_nonzero(np.isnan(np_forest_job)))\n",
    "print(np.min(np_forest_job))\n",
    "print(np.max(np_forest_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6037ee6-f94a-4b06-beec-c7ddd4805e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pixel_area = csv_dir / \"pixel_area.csv\"\n",
    "np_pixel_area = np.genfromtxt(pixel_area, delimiter=\",\")\n",
    "print(np.min(np_pixel_area))\n",
    "print(np.max(np_pixel_area))\n",
    "print(np.mean(np_pixel_area))\n",
    "print(np.count_nonzero(np.isnan(np_pixel_area)))\n",
    "print(len(np_pixel_area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a527d-2338-45ca-8923-4e8c59400495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "potential_treecover = csv_dir / \"potential_treecover.csv\"\n",
    "potential_treecover = np.genfromtxt(potential_treecover, delimiter=\",\")\n",
    "print(np.nanmin(potential_treecover))\n",
    "print(np.nanmax(potential_treecover))\n",
    "print(np.count_nonzero(np.isnan(potential_treecover)))\n",
    "print(len(potential_treecover))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804dc4d1-d49e-4ff9-af8a-8ea0f81cd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "50138417 / 106816099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a358fc-c11c-419c-b0bb-76371dc9b3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
