{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb3255-b347-4cf5-99d8-ae37bd64fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the downloading path\n",
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path.home() / \"module_results\" / \"se.plan\"\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "save_dir.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d8aca3-5174-4c90-94ef-496d16bcf3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the country list\n",
    "import pandas as pd\n",
    "from component import parameter as pm\n",
    "\n",
    "lmic_list = pd.read_csv(pm.country_list)\n",
    "\n",
    "lmic_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bdce5-d76b-48bb-9e17-2028911cfd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the gadm country list\n",
    "from sepal_ui import aoi\n",
    "from sepal_ui import sepalwidgets as sw\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "aoi_model = aoi.AoiModel(alert=sw.Alert(), gee=False)\n",
    "\n",
    "country_list = pd.read_csv(aoi_model.FILE[0])  # GADM based\n",
    "\n",
    "\n",
    "def append_gdf(admin=None):\n",
    "    \"\"\"append the gdf of the differnet LMIC areas if possible\"\"\"\n",
    "\n",
    "    # get all the sub administrative areas\n",
    "    df = country_list[country_list.GID_0 == admin]  # only the featured country\n",
    "    df = df.drop_duplicates(subset=\"GID_1\")  # remove all GID_2\n",
    "\n",
    "    # get all the admin numbers\n",
    "    gdf = None\n",
    "    for i, row in df.iterrows():\n",
    "\n",
    "        admin = row.GID_1 if row.GID_1 and row.GID_1 == row.GID_1 else row.GID_0\n",
    "\n",
    "        aoi_model.admin = admin\n",
    "        aoi_model.set_object(method=\"ADMIN0\")\n",
    "\n",
    "        tmp_gdf = aoi_model.gdf\n",
    "        gdf = tmp_gdf if type(gdf) == type(None) else gdf.append(tmp_gdf)\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8598b5-aeed-40dc-b05e-38359be976f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf_adm1_path = save_dir / \"lmic_leve1.shp\"\n",
    "\n",
    "if gdf_adm1_path.is_file():\n",
    "\n",
    "    # read the file\n",
    "    gdf = gpd.read_file(gdf_adm1_path)\n",
    "\n",
    "else:\n",
    "    gdf = None\n",
    "    with tqdm(total=len(lmic_list)) as pbar:\n",
    "        for i, row in lmic_list.iterrows():\n",
    "\n",
    "            tmp_gdf = append_gdf(row.ISO3)\n",
    "            gdf = tmp_gdf if type(gdf) == type(None) else gdf.append(tmp_gdf)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    gdf.to_file(gdf_adm1_path)\n",
    "\n",
    "len(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354220cc-155b-4532-a117-12f5e06e3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gdf.GID_0.unique()) == 139  ## check that all the countries are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60814699-6d85-4678-8d85-37be4db3d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the layers\n",
    "import ee\n",
    "import pandas as pd\n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "from component import parameter as pm\n",
    "\n",
    "layer_list = (\n",
    "    pd.read_csv(pm.layer_list)\n",
    "    .filter(items=[\"layer_id\", \"theme\", \"gee_asset\"])\n",
    "    .sort_values(by=[\"theme\"])\n",
    ")\n",
    "layer_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7730a3-0def-4260-b230-f8e26f48f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the reference parameters\n",
    "name = \"treecover_with_potential\"\n",
    "layer = layer_list[layer_list.layer_id == name]\n",
    "\n",
    "ee_ref = ee.Image(layer.iloc[0].gee_asset)\n",
    "ee_ref_crs = ee_ref.projection()\n",
    "ee_ref_geom = ee_ref.geometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7762d-9215-4a4c-ada8-b2a8f1ce9688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export all the layers witht the adapted reducer\n",
    "from component.scripts import gdrive\n",
    "\n",
    "drive_handler = gdrive()\n",
    "with tqdm(total=len(layer_list)) as pbar:\n",
    "    for i, row in layer_list.iterrows():\n",
    "\n",
    "        ee_image = None\n",
    "        ee_reducer = None\n",
    "        if row.layer_id in [\"protected_areas\"]:  # rasterize before export\n",
    "\n",
    "            ee_reducer = ee.Reducer.allNonZero()\n",
    "            ee_image = (\n",
    "                ee.FeatureCollection(row.gee_asset.strip())\n",
    "                .filter(ee.Filter.neq(\"WDPAID\", {}))\n",
    "                .reduceToImage(properties=[\"WDPAID\"], reducer=ee.Reducer.allNonZero())\n",
    "                .gt(0)\n",
    "                .unmask(0)\n",
    "                .rename(\"wdpa\")\n",
    "                .select(\"wdpa\")\n",
    "                .reproject(ee_ref_crs)\n",
    "            )\n",
    "\n",
    "        elif row.layer_id in [\n",
    "            \"treecover_with_potential\",\n",
    "            \"declining_population\",\n",
    "        ]:  # binaries inclusive\n",
    "\n",
    "            ee_reducer = ee.Reducer.anyNonZero()\n",
    "\n",
    "        elif row.layer_id in [\"ecozones\", \"land_cover\"]:  # most frequent value\n",
    "            ee_reducer = \"mode\"\n",
    "\n",
    "        ee_image = ee_image if ee_image else ee.Image(row.gee_asset.strip()).select(0)\n",
    "        ee_reducer = ee_reducer if ee_reducer else ee.Reducer.mean()\n",
    "\n",
    "        # export\n",
    "        if (\n",
    "            not len(drive_handler.get_files(row.layer_id))\n",
    "            and not (save_dir / f\"{row.layer_id}.vrt\").is_file()\n",
    "        ):\n",
    "            task_config = {\n",
    "                \"folder\": \"se.plan\",\n",
    "                \"image\": ee_image.reduceResolution(reducer=ee_reducer, maxPixels=2048),\n",
    "                \"description\": row.layer_id,\n",
    "                \"region\": ee_ref_geom,\n",
    "                \"scale\": 1000,\n",
    "                \"crs\": ee_ref_crs,\n",
    "                \"maxPixels\": 10e12,\n",
    "            }\n",
    "\n",
    "            task = ee.batch.Export.image.toDrive(**task_config)\n",
    "            task.start()\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "print(\n",
    "    \"You can now monitor your exporting steps from earthegine: https://code.earthengine.google.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce17f06-414c-4068-8287-aec44b9f0f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download treecover files\n",
    "# add them in the same directory\n",
    "# add them to the layer list\n",
    "\n",
    "ee_treecover = ee.Image(\"COPERNICUS/Landcover/100m/Proba-V-C3/Global/2019\").select(\n",
    "    \"tree-coverfraction\"\n",
    ")\n",
    "\n",
    "treecover = \"curent_treecover\"\n",
    "# export\n",
    "if (\n",
    "    not len(drive_handler.get_files(treecover))\n",
    "    and not (save_dir / f\"{treecover}.vrt\").is_file()\n",
    "):\n",
    "    task_config = {\n",
    "        \"folder\": \"se.plan\",\n",
    "        \"image\": ee_image,\n",
    "        \"description\": treecover,\n",
    "        \"region\": ee_ref_geom,\n",
    "        \"scale\": 1000,\n",
    "        \"crs\": ee_ref_crs,\n",
    "        \"maxPixels\": 10e12,\n",
    "    }\n",
    "\n",
    "    task = ee.batch.Export.image.toDrive(**task_config)\n",
    "    task.start()\n",
    "\n",
    "# add to the layer_list\n",
    "if not treecover in layer_list.layer_id:\n",
    "    layer_list.loc[len(layer_list)] = [treecover, \"treecover\", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f406d-d1e4-4c53-b210-82419b34e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once the image are available on my drive\n",
    "# download theme as files\n",
    "from pathlib import Path\n",
    "from osgeo import gdal\n",
    "\n",
    "raw_dir = save_dir / \"raw_data\"\n",
    "raw_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with tqdm(total=len(layer_list)) as pbar:\n",
    "    for i, row in layer_list.iterrows():\n",
    "\n",
    "        vrt_path = save_dir / f\"{row.layer_id}.vrt\"\n",
    "\n",
    "        if vrt_path.is_file():\n",
    "            pbar.update()\n",
    "            continue\n",
    "\n",
    "        files = drive_handler.get_files(row.layer_id)\n",
    "        if len(files):\n",
    "            loc_files = drive_handler.download_files(files, raw_dir)\n",
    "            drive_handler.delete_files(files)\n",
    "\n",
    "            # create a vrt to manipulate everything\n",
    "            ds = gdal.BuildVRT(str(vrt_path), [str(f) for f in loc_files])\n",
    "            ds.FlushCache()\n",
    "\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ae04b0-915d-4ff6-99da-f6b791aa173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the bastin dataset\n",
    "# add the dataset to the raw folder under total_potential.tif file\n",
    "import rasterio as rio\n",
    "from rasterio import warp\n",
    "\n",
    "# align the dataset on the others\n",
    "def align(src_rst, template_rst, out_rst, verbose=False):\n",
    "    \"\"\"Align a source raster on a template\n",
    "\n",
    "    Args :\n",
    "        src_rst (str) : path to the source raster\n",
    "        template_rst (str) : path to the template raster\n",
    "        out_rst (str) : path to the output raster\n",
    "\n",
    "    Return :\n",
    "        out_rst\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # get template crs and transform\n",
    "    with rio.open(template_rst) as tplt, rio.open(src_rst) as src:\n",
    "\n",
    "        transform, width, height = warp.calculate_default_transform(\n",
    "            src.crs, tplt.crs, tplt.width, tplt.height, *tplt.bounds\n",
    "        )\n",
    "\n",
    "        kwargs = src.meta.copy()\n",
    "\n",
    "        kwargs.update(\n",
    "            driver=\"GTiff\",\n",
    "            height=height,\n",
    "            width=width,\n",
    "            transform=transform,\n",
    "            compress=\"lzw\",\n",
    "        )\n",
    "\n",
    "        # destination\n",
    "        with rio.open(out_rst, \"w\", **kwargs) as dst:\n",
    "            for i in range(1, dst.count + 1):\n",
    "                warp.reproject(\n",
    "                    source=rio.band(src, i),\n",
    "                    destination=rio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=tplt.crs,\n",
    "                    resampling=warp.Resampling.bilinear,\n",
    "                )\n",
    "\n",
    "    print(f\"The raster {src_rst} has been align on {template_rst} in {out_rst}\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "name = \"potential_treecover\"\n",
    "input_tif = raw_dir / \"Total_potential.tif\"\n",
    "output_tif = raw_dir / f\"{name}.tif\"\n",
    "template_vrt = save_dir / \"treecover_with_potential.vrt\"\n",
    "\n",
    "if not output_tif.is_file():\n",
    "    align(input_tif, template_vrt, output_tif)\n",
    "\n",
    "    # create a binding vrt\n",
    "    vrt_path = save_dir / f\"{name}.vrt\"\n",
    "    ds = gdal.BuildVRT(str(vrt_path), [str(output_tif)])\n",
    "    ds.FlushCache()\n",
    "\n",
    "if not name in layer_list.layer_id:\n",
    "    layer_list.loc[len(layer_list)] = [name, \"treecover\", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a4df8a-32dc-4c99-a29c-69f405454d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(layer_list) == 25  # initial number + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd15e9-b3a2-4f20-b23b-164454245071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all the layers into 1 single vrt\n",
    "global_vrt_path = save_dir / \"layers.vrt\"\n",
    "\n",
    "layer_files = [\n",
    "    str(save_dir / f\"{row.layer_id}.vrt\") for _, row in layer_list.iterrows()\n",
    "]\n",
    "ds = gdal.BuildVRT(str(global_vrt_path), layer_files, separate=True)\n",
    "ds.FlushCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4196c-c22f-4b92-af97-4993c994206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "\n",
    "# read the mask layer\n",
    "with rio.open(save_dir / \"treecover_with_potential.vrt\") as mask_f:\n",
    "    mask = mask_f.read(1)\n",
    "\n",
    "    # count the number of unmask values\n",
    "    nb_unmasked = np.sum(mask)\n",
    "\n",
    "nb_unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf890c2e-ee67-430c-aebd-408ef0fc6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "from rasterio.windows import Window\n",
    "from itertools import product\n",
    "from shapely import geometry as sg\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# create the csv\n",
    "csv_f = save_dir / \"world_Data_restoration_1km.csv\"\n",
    "\n",
    "if not csv_f.is_file():\n",
    "\n",
    "    # read the mask layer\n",
    "    with rio.open(save_dir / \"treecover_with_potential.vrt\") as mask_f:\n",
    "        mask = mask_f.read(1)\n",
    "\n",
    "        # count the number of unmask values\n",
    "        nb_unmasked = np.sum(mask)\n",
    "\n",
    "        # init the columns\n",
    "        # allocate the array for efficiency\n",
    "        columns = layer_list.layer_id.to_list()\n",
    "        data = {\n",
    "            \"long\": np.empty(nb_unmasked),\n",
    "            \"lat\": np.empty(nb_unmasked),\n",
    "            \"ADM_0\": np.empty(nb_unmasked, dtype=str),\n",
    "            \"ADM_1\": np.empty(nb_unmasked, dtype=str),\n",
    "        }\n",
    "\n",
    "        with tqdm(total=mask_f.height * mask_f.width) as pbar:\n",
    "            for i, xy in enumerate(product(range(mask_f.height), range(mask_f.width))):\n",
    "\n",
    "                # expand\n",
    "                x, y = xy\n",
    "\n",
    "                # don't do anything if it's masked\n",
    "                if mask[x][y] == 0:\n",
    "\n",
    "                    data[\"long\"][i] = np.nan\n",
    "                    data[\"lat\"][i] = np.nan\n",
    "\n",
    "                    data[\"ADM_0\"][i] = \"\"\n",
    "                    data[\"ADM_1\"][i] = \"\"\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # get the coordinates\n",
    "                    coords = rio.transform.xy(\n",
    "                        mask_f.profile[\"transform\"], x, y, offset=\"center\"\n",
    "                    )  # x is lat\n",
    "                    data[\"long\"][i] = coords[1]\n",
    "                    data[\"lat\"][i] = coords[0]\n",
    "\n",
    "                    # get the administrative values\n",
    "                    point = sg.Point(coords[::-1])\n",
    "                    min_ = gdf.distance(point).min()\n",
    "                    row = gdf[gdf.distance(point) == min_].iloc[0]\n",
    "                    data[\"ADM_0\"][i] = row.NAME_0\n",
    "                    data[\"ADM_1\"][i] = row.NAME_1\n",
    "\n",
    "                pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb7d59-7755-4e72-8550-250d88c6c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "import rioxarray\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "# if not csv_f.is_fiel():\n",
    "\n",
    "# add data to the data dict\n",
    "mask = rioxarray.open_rasterio(\n",
    "    save_dir / \"treecover_with_potential.vrt\", chunks=5\n",
    ").astype(int)\n",
    "print(mask.shape)\n",
    "\n",
    "for _, row in layer_list.iterrows():\n",
    "    rds = rioxarray.open_rasterio(save_dir / f\"{row.layer_id}.vrt\", chunks=5)\n",
    "    rds.drop(\"spatial_ref\").drop(\"band\")\n",
    "    rds = rds.where(mask == 1)\n",
    "    rds.name = row.layer_id\n",
    "    df = rds.to_dataframe().reset_index()\n",
    "\n",
    "    break\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927762a-dcb2-4bf4-aa40-553bda3f737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "from rasterio.windows import Window\n",
    "from itertools import product\n",
    "from shapely import geometry as sg\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# create the csv\n",
    "csv_f = save_dir / \"world_Data_restoration_1km.csv\"\n",
    "\n",
    "if not csv_f.is_file():\n",
    "\n",
    "    # read the mask layer\n",
    "    with rio.open(save_dir / \"treecover_with_potential.vrt\") as mask_f:\n",
    "        mask = mask_f.read(1)\n",
    "\n",
    "        # count the number of unmask values\n",
    "        nb_unmasked = np.sum(mask)\n",
    "\n",
    "        # init the columns\n",
    "        # allocate the array for efficiency\n",
    "        columns = layer_list.layer_id.to_list()\n",
    "        data = {\n",
    "            \"long\": np.empty(nb_unmasked),\n",
    "            \"lat\": np.empty(nb_unmasked),\n",
    "            \"ADM_0\": np.empty(nb_unmasked, dtype=str),\n",
    "            \"ADM_1\": np.empty(nb_unmasked, dtype=str),\n",
    "        }\n",
    "        data = {**data, **{k: np.empty(nb_unmasked) for k in columns}}\n",
    "\n",
    "        with tqdm(total=mask_f.height * mask_f.width) as pbar:\n",
    "            count = 0\n",
    "            for x, y in product(range(mask_f.height), range(mask_f.width)):\n",
    "\n",
    "                # don't do anything if it's masked\n",
    "                if mask[x][y] != 0:\n",
    "\n",
    "                    start = time.time()\n",
    "                    # get the coordinates\n",
    "                    coords = rio.transform.xy(\n",
    "                        mask_f.profile[\"transform\"], x, y, offset=\"center\"\n",
    "                    )  # x is lat\n",
    "                    data[\"long\"][count] = coords[1]\n",
    "                    data[\"lat\"][count] = coords[0]\n",
    "                    print(f\"get coords: {time.time()-start}s\")\n",
    "\n",
    "                    start = time.time()\n",
    "                    # get the administrative values\n",
    "                    point = sg.Point(coords[::-1])\n",
    "                    min_ = gdf.distance(point).min()\n",
    "                    row = gdf[gdf.distance(point) == min_].iloc[0]\n",
    "                    data[\"ADM_0\"][count] = row.NAME_0\n",
    "                    data[\"ADM_1\"][count] = row.NAME_1\n",
    "                    print(f\"get admin: {time.time()-start}s\")\n",
    "\n",
    "                    start_layer = time.time()\n",
    "                    # get the value of all the other layers\n",
    "                    window = Window(y, x, 1, 1)\n",
    "                    with rio.open(global_vrt_path) as f:\n",
    "\n",
    "                        for i, column in enumerate(columns):\n",
    "                            data[column][count] = f.read(i + 1, window=window)[0][0]\n",
    "\n",
    "                    print(f\"get all layers: {time.time()-start_layer}s\")\n",
    "\n",
    "                    # update the count value\n",
    "                    count += 1\n",
    "\n",
    "                    break\n",
    "\n",
    "                pbar.update()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a811652f-b956-4ebc-bd8b-be06752e53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the df from it\n",
    "if not csv_f.is_file():\n",
    "    df = pd.DataFrame(data)\n",
    "else:\n",
    "    df = pd.read_csv(csv_f)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b10c7-cd8e-4095-a58b-3d6a0c44ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(save_dir / \"world_Data_restoration_1km.csv\")\n",
    "# df.to_csv(csv_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
